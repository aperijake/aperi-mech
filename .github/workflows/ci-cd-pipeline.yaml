name: CI/CD Pipeline

# Hybrid CI/CD Pipeline with VM Pool:
# - CPU tests: Run in parallel on GitHub-hosted runners (~25 min)
# - GPU tests: Run in parallel across 3 Azure VMs (~25-35 min)
# - VMs use persistent OS disks (32GB Standard SSD), dynamic IPs, temp storage for Docker
# - Cost: ~$20-24/month (compute ~$13-17 + persistent OS disks $7.20)
# - Total runtime: ~35 min (76% faster than 145 min serial, 50% faster than single VM)
# - To scale pool: Search for "VM_POOL_CONFIG" and update all marked sections

on:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main
  workflow_dispatch: ~ # Allows manual triggering of the workflow

permissions:
  contents: read
  packages: read # For pulling from GHCR

concurrency:
  group: ci-cd-hybrid-${{ github.ref }}
  cancel-in-progress: true

jobs:
  #############################################################################
  # CPU Tests - Run on GitHub-hosted runners (parallel)
  #############################################################################

  build-and-test-cpu:
    name: Build and Test CPU (${{ matrix.config.BUILD_TYPE }}, PROTEGO=${{ matrix.config.PROTEGO }})
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      max-parallel: 4 # Run all 4 CPU configs in parallel
      matrix:
        config:
          - { BUILD_TYPE: Debug, PROTEGO: true }
          - { BUILD_TYPE: Debug, PROTEGO: false }
          - { BUILD_TYPE: Release, PROTEGO: true }
          - { BUILD_TYPE: Release, PROTEGO: false }

    container:
      image: ghcr.io/aperijake/aperi-mech:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
      options: --user root

    env:
      BUILD_TYPE: ${{ matrix.config.BUILD_TYPE }}
      PROTEGO: ${{ matrix.config.PROTEGO }}
      # MPI Configuration for Docker Containers:
      # - OMPI_ALLOW_RUN_AS_ROOT: Required because GitHub Actions containers run as root user
      OMPI_ALLOW_RUN_AS_ROOT: 1
      OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: 1
      # - PRTE_MCA_rmaps: Allow oversubscription since container reports only 1 slot despite having 4 cores
      PRTE_MCA_rmaps_default_mapping_policy: :oversubscribe
      # - OMPI_MCA_btl: Disable shared memory transport to avoid container restrictions, use TCP loopback instead
      OMPI_MCA_btl: ^sm
      OMPI_MCA_btl_base_warn_component_unused: 0

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true
          submodules: ${{ matrix.config.PROTEGO == true && 'recursive' || 'false' }}
          ssh-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Fix git ownership
        shell: bash
        run: |
          # Fix git ownership warning (Actions checks out as different user)
          git config --global --add safe.directory $GITHUB_WORKSPACE

      - name: Configure and build
        shell: bash
        run: |
          set -e

          # GitHub Actions checks out code to $GITHUB_WORKSPACE
          # We need to work from there, not the container's $APERI_MECH_ROOT
          cd $GITHUB_WORKSPACE

          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          # Determine build path and flags
          BUILD_ROOT="build"
          CONFIGURE_FLAGS=""
          if [ "$PROTEGO" = "true" ]; then
            BUILD_ROOT="protego-mech/build"
            CONFIGURE_FLAGS="--protego-mech"
          fi
          BUILD_PATH="${BUILD_ROOT}/${BUILD_TYPE}"

          # Debug info
          echo "============================================"
          echo "Build Environment Info:"
          echo "  CPU cores (nproc): $(nproc)"
          echo "  CPU cores (nproc --all): $(nproc --all)"
          echo "  Memory: $(free -h | grep Mem | awk '{print $2}')"
          echo "  Build path: ${BUILD_PATH}"
          echo "  Configure flags: ${CONFIGURE_FLAGS}"
          echo "============================================"

          # Configure
          ./do_configure --build-type ${BUILD_TYPE} ${CONFIGURE_FLAGS}

          # Build using all available cores
          # Note: Use 'nproc --all' instead of 'nproc' because containers may report limited quota
          # but actually have access to all host cores for build parallelism
          echo "Building with $(nproc --all) parallel jobs"
          cd ${BUILD_PATH}
          make -j$(nproc --all)

      - name: Run material tests
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          BUILD_ROOT="build"
          if [ "$PROTEGO" = "true" ]; then
            BUILD_ROOT="protego-mech/build"
          fi
          BUILD_PATH="${BUILD_ROOT}/${BUILD_TYPE}"

          cd ${BUILD_PATH}
          echo "Running material tests..."
          make run_material_tests

      - name: Run utils modules tests
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          BUILD_ROOT="build"
          if [ "$PROTEGO" = "true" ]; then
            BUILD_ROOT="protego-mech/build"
          fi
          BUILD_PATH="${BUILD_ROOT}/${BUILD_TYPE}"

          cd ${BUILD_PATH}

          # Add aperi-mech build directory to PATH (needed for utils tests to find aperi-mech executable)
          export PATH=${GITHUB_WORKSPACE}/${BUILD_PATH}:$PATH

          echo "Running utils modules tests..."
          echo "aperi-mech location: $(which aperi-mech || echo 'NOT FOUND')"
          echo "exodiff location: $(which exodiff || echo 'NOT FOUND')"
          make run_utils_modules_tests

      - name: Run unit tests (serial)
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          BUILD_ROOT="build"
          if [ "$PROTEGO" = "true" ]; then
            BUILD_ROOT="protego-mech/build"
          fi
          BUILD_PATH="${BUILD_ROOT}/${BUILD_TYPE}"

          cd ${BUILD_PATH}
          # Check if unit_tests executable exists
          if [ ! -f "./unit_tests" ]; then
            echo "unit_tests executable not found, skipping..."
            exit 0
          fi
          mpirun --allow-run-as-root -n 1 ./unit_tests

      - name: Run unit tests (parallel)
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          BUILD_ROOT="build"
          if [ "$PROTEGO" = "true" ]; then
            BUILD_ROOT="protego-mech/build"
          fi
          BUILD_PATH="${BUILD_ROOT}/${BUILD_TYPE}"

          cd ${BUILD_PATH}
          # Check if unit_tests executable exists
          if [ ! -f "./unit_tests" ]; then
            echo "unit_tests executable not found, skipping..."
            exit 0
          fi
          mpirun --allow-run-as-root -n 3 ./unit_tests

      - name: Run regression tests (serial)
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          # Determine build directory and test selection file
          BUILD_DIR="build"
          BUILD_FLAG="--${BUILD_TYPE,,}"  # Convert to lowercase for flag
          CPU_FLAG="--cpu"  # GitHub runners don't have GPU
          TEST_PATHS_FILE="test/regression_tests/regression_test_paths.yaml"

          if [ "$PROTEGO" = "true" ]; then
            BUILD_DIR="protego-mech/build"
            TEST_PATHS_FILE="protego-mech/test/regression_tests/regression_test_paths.yaml"
          fi

          echo "Running regression tests (serial)..."
          python3 test/run_regression_tests.py --build-dir ${BUILD_DIR} --directory-file ${TEST_PATHS_FILE} ${BUILD_FLAG} ${CPU_FLAG} --serial

      - name: Run regression tests (parallel)
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          # Determine build directory and test selection file
          BUILD_DIR="build"
          BUILD_FLAG="--${BUILD_TYPE,,}"  # Convert to lowercase for flag
          CPU_FLAG="--cpu"  # GitHub runners don't have GPU
          TEST_PATHS_FILE="test/regression_tests/regression_test_paths.yaml"

          if [ "$PROTEGO" = "true" ]; then
            BUILD_DIR="protego-mech/build"
            TEST_PATHS_FILE="protego-mech/test/regression_tests/regression_test_paths.yaml"
          fi

          echo "Running regression tests (parallel)..."
          python3 test/run_regression_tests.py --build-dir ${BUILD_DIR} --directory-file ${TEST_PATHS_FILE} ${BUILD_FLAG} ${CPU_FLAG} --parallel

      - name: Run GUI tests
        if: env.PROTEGO == 'true'
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE/protego-mech/gui

          # Use the container's existing Python virtual environment (has all GUI dependencies)
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate

          # Install test dependencies
          pip install -q pytest pytest-cov

          # Add gui package to Python path for imports
          # Note: We don't use setup.py/setup_linux.py as they're for AppImage building, not pip installation
          export PYTHONPATH=$GITHUB_WORKSPACE/protego-mech/gui:$PYTHONPATH

          # Run tests
          cd test
          pytest -v test_image_segmenter.py test_meshing.py test_simulation.py

  #############################################################################
  # GPU Tests - Run on Azure VM Pool (parallel across 3 VMs)
  #############################################################################
  # VM_POOL_CONFIG: Current pool size is 3 VMs
  # To scale the pool, search for "VM_POOL_CONFIG" and update all marked sections
  #############################################################################

  setup-gpu:
    name: Setup GPU VM (${{ matrix.vm_index }})
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      max-parallel: 3 # VM_POOL_CONFIG: Start all VMs in parallel (set to pool size)
      matrix:
        vm_index: [1, 2, 3] # VM_POOL_CONFIG: Update array when adding/removing VMs

    steps:
      - name: Checkout Code on Runner
        uses: actions/checkout@v4
        with:
          lfs: false
          ref: ${{ github.head_ref }}

      - name: Start Azure VM and Get Status
        id: vm-status
        uses: azure/CLI@v2
        with:
          azcliversion: 2.62.0
          inlineScript: |
            echo "Logging into Azure..."
            az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }}
            if [ $? -eq 0 ]; then
              echo "Logged in successfully"
            else
              echo "Failed to login"
              exit 1
            fi

            echo "Setting Azure subscription..."
            az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}
            if [ $? -eq 0 ]; then
                echo "Subscription set successfully"
            else
                echo "Failed to set subscription"
                exit 1
            fi

            # Determine VM name based on matrix index
            if [ "${{ matrix.vm_index }}" = "1" ]; then
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}"
            else
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}-${{ matrix.vm_index }}"
            fi

            echo "Checking VM: $VM_NAME"
            VM_STATUS=$(az vm get-instance-view --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME" --query instanceView.statuses[1].displayStatus --output tsv)

            if [ "$VM_STATUS" != "VM running" ]; then
              echo "Starting VM $VM_NAME..."
              az vm start --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME"
              echo "vm_started_${{ matrix.vm_index }}=true" >> $GITHUB_OUTPUT
            else
              echo "VM $VM_NAME is already running"
              echo "vm_started_${{ matrix.vm_index }}=false" >> $GITHUB_OUTPUT
            fi

      - name: Get VM Dynamic IP
        id: get-ip
        uses: azure/CLI@v2
        with:
          azcliversion: 2.62.0
          inlineScript: |
            # Login (reuse credentials from previous step)
            az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }}
            az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}

            # Determine VM name
            if [ "${{ matrix.vm_index }}" = "1" ]; then
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}"
            else
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}-${{ matrix.vm_index }}"
            fi

            # Get dynamic public IP with retry logic
            echo "Querying public IP for $VM_NAME..."
            for i in {1..5}; do
              VM_IP=$(az vm show -d --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME" --query publicIps -o tsv)
              if [ -n "$VM_IP" ] && [ "$VM_IP" != "null" ]; then
                echo "VM $VM_NAME Public IP: $VM_IP"
                echo "vm_ip_${{ matrix.vm_index }}=$VM_IP" >> $GITHUB_OUTPUT
                exit 0
              fi
              echo "Retry $i: Waiting for IP address..."
              sleep 10
            done

            echo "Error: Could not retrieve IP for $VM_NAME"
            exit 1

      - name: Setup SSH for Dynamic IP
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

          # Get IP from previous step output
          VM_IP="${{ steps.get-ip.outputs[format('vm_ip_{0}', matrix.vm_index)] }}"

          # Add host key using ssh-keyscan (for dynamic IPs)
          echo "Adding SSH host key for $VM_IP..."
          ssh-keyscan -H "$VM_IP" >> ~/.ssh/known_hosts 2>/dev/null || true

          # Test SSH connection
          ssh -T -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${{ secrets.VM_USERNAME }}@"$VM_IP" "echo 'SSH connection successful'"

      - name: Setup Build Directories on VM
        run: |
          # VM_POOL_CONFIG: Map VM index to IP output (add cases when scaling)
          case "${{ matrix.vm_index }}" in
            1) VM_IP="${{ steps.get-ip.outputs.vm_ip_1 }}" ;;
            2) VM_IP="${{ steps.get-ip.outputs.vm_ip_2 }}" ;;
            3) VM_IP="${{ steps.get-ip.outputs.vm_ip_3 }}" ;;
            *) echo "Error: Invalid VM index ${{ matrix.vm_index }}"; exit 1 ;;
          esac

          ssh -T -o ConnectTimeout=10 ${{ secrets.VM_USERNAME }}@"$VM_IP" << 'EOF'
            set -e

            # Create build directories on temp storage
            mkdir -p /mnt/builds
            mkdir -p /mnt/builds/protego-builds

            # Clean up any old builds
            rm -rf /mnt/builds/*
            rm -rf /mnt/builds/protego-builds/*

            echo "Build directories ready on temp storage"
          EOF

  # Intermediate job to query VM IPs after they're started
  get-vm-ips:
    name: Get VM IPs
    runs-on: ubuntu-latest
    needs: setup-gpu

    outputs:
      # VM_POOL_CONFIG: Add vm_ip_N output for each VM in the pool
      vm_ip_1: ${{ steps.get-ips.outputs.vm_ip_1 }}
      vm_ip_2: ${{ steps.get-ips.outputs.vm_ip_2 }}
      vm_ip_3: ${{ steps.get-ips.outputs.vm_ip_3 }}

    steps:
      - name: Query All VM IPs
        id: get-ips
        uses: azure/CLI@v2
        with:
          azcliversion: 2.62.0
          inlineScript: |
            echo "Logging into Azure..."
            az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }}
            az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}

            # VM_POOL_CONFIG: Update loop range to match pool size (e.g., "1 2 3 4" for 4 VMs)
            for i in 1 2 3; do
              if [ "$i" = "1" ]; then
                VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}"
              else
                VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}-$i"
              fi

              echo "Querying IP for $VM_NAME..."
              VM_IP=$(az vm show -d --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME" --query publicIps -o tsv)

              if [ -n "$VM_IP" ] && [ "$VM_IP" != "null" ]; then
                echo "VM $VM_NAME IP: $VM_IP"
                echo "vm_ip_$i=$VM_IP" >> $GITHUB_OUTPUT
              else
                echo "Warning: Could not get IP for $VM_NAME"
              fi
            done

  build-and-test-gpu:
    name: Build and Test GPU (${{ matrix.config.BUILD_TYPE }}, PROTEGO=${{ matrix.config.PROTEGO }}, VM=${{ matrix.config.VM_INDEX }})
    runs-on: ubuntu-latest
    needs: get-vm-ips

    strategy:
      fail-fast: false
      max-parallel: 3 # VM_POOL_CONFIG: Set to pool size to run one config per VM
      matrix:
        # VM_POOL_CONFIG: Assign each test config to a VM_INDEX
        # With 3 VMs and 4 configs, VM 1 runs 2 configs sequentially
        # With 4 VMs and 4 configs, each VM runs 1 config (true 1:1 parallelization)
        config:
          - { BUILD_TYPE: Debug, GPU: true, PROTEGO: true, VM_INDEX: 1 }
          - { BUILD_TYPE: Debug, GPU: true, PROTEGO: false, VM_INDEX: 2 }
          - { BUILD_TYPE: Release, GPU: true, PROTEGO: true, VM_INDEX: 3 }
          - { BUILD_TYPE: Release, GPU: true, PROTEGO: false, VM_INDEX: 1 }

    env:
      BUILD_TYPE: ${{ matrix.config.BUILD_TYPE }}
      GPU: ${{ matrix.config.GPU }}
      PROTEGO: ${{ matrix.config.PROTEGO }}

    steps:
      - name: Set VM IP
        id: set-ip
        run: |
          # VM_POOL_CONFIG: Map config VM_INDEX to IP output (add cases when scaling)
          case "${{ matrix.config.VM_INDEX }}" in
            1) VM_IP="${{ needs.get-vm-ips.outputs.vm_ip_1 }}" ;;
            2) VM_IP="${{ needs.get-vm-ips.outputs.vm_ip_2 }}" ;;
            3) VM_IP="${{ needs.get-vm-ips.outputs.vm_ip_3 }}" ;;
            *) echo "Error: Invalid VM index ${{ matrix.config.VM_INDEX }}"; exit 1 ;;
          esac
          echo "VM_IP=$VM_IP" >> $GITHUB_ENV
          echo "vm_ip=$VM_IP" >> $GITHUB_OUTPUT
          echo "Using VM IP: $VM_IP for VM ${{ matrix.config.VM_INDEX }}"

      - name: Checkout Code on Runner
        uses: actions/checkout@v4
        with:
          lfs: false
          ref: ${{ github.head_ref }}

      - name: Setup SSH for Dynamic IP
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

          # Add host key using ssh-keyscan (for dynamic IPs)
          echo "Adding SSH host key for ${{ steps.set-ip.outputs.vm_ip }}..."
          ssh-keyscan -H "${{ steps.set-ip.outputs.vm_ip }}" >> ~/.ssh/known_hosts 2>/dev/null || true

          # Test SSH connection
          ssh -T -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${{ secrets.VM_USERNAME }}@"${{ steps.set-ip.outputs.vm_ip }}" "echo 'SSH connection successful to VM ${{ matrix.config.VM_INDEX }}'"

      - name: Common Runner Config Steps
        uses: ./.github/actions/common-runner-config-steps

      - name: Build
        uses: ./.github/actions/build-action
        with:
          build-type: ${{ env.BUILD_TYPE }}
          VM_IP: ${{ steps.set-ip.outputs.vm_ip }}
          SSH_PRIVATE_KEY: ${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}
          VM_USERNAME: ${{ secrets.VM_USERNAME }}
          VM_KNOWN_HOST: ${{ secrets.VM_KNOWN_HOST }}
          gpu: ${{ env.GPU }}
          with-protego: ${{ env.PROTEGO }}
          vm-pool: "true"
          CICD_REPO_SECRET: ${{ secrets.CICD_REPO_SECRET }}

      - name: Run material tests
        uses: ./.github/actions/run-material-tests
        with:
          build-type: ${{ env.BUILD_TYPE }}
          VM_IP: ${{ steps.set-ip.outputs.vm_ip }}
          SSH_PRIVATE_KEY: ${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}
          VM_USERNAME: ${{ secrets.VM_USERNAME }}
          gpu: ${{ env.GPU }}
          with-protego: ${{ env.PROTEGO }}
          vm-pool: "true"

      - name: Run utils modules tests
        uses: ./.github/actions/run-utils-modules-tests
        with:
          build-type: ${{ env.BUILD_TYPE }}
          VM_IP: ${{ steps.set-ip.outputs.vm_ip }}
          SSH_PRIVATE_KEY: ${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}
          VM_USERNAME: ${{ secrets.VM_USERNAME }}
          gpu: ${{ env.GPU }}
          with-protego: ${{ env.PROTEGO }}
          vm-pool: "true"

      - name: Run unit tests
        uses: ./.github/actions/run-unit-tests
        with:
          build-type: ${{ env.BUILD_TYPE }}
          VM_IP: ${{ steps.set-ip.outputs.vm_ip }}
          SSH_PRIVATE_KEY: ${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}
          VM_USERNAME: ${{ secrets.VM_USERNAME }}
          gpu: ${{ env.GPU }}
          num-processes: 1
          with-protego: ${{ env.PROTEGO }}
          vm-pool: "true"

      - name: Run regression tests, serial
        uses: ./.github/actions/run-regression-tests
        with:
          VM_IP: ${{ steps.set-ip.outputs.vm_ip }}
          SSH_PRIVATE_KEY: ${{ secrets.AZURE_CICD_GPU_VM_PRIVATE_KEY }}
          VM_USERNAME: ${{ secrets.VM_USERNAME }}
          gpu: ${{ env.GPU }}
          parallel: false
          build: ${{ env.BUILD_TYPE }}
          with-protego: ${{ env.PROTEGO }}
          vm-pool: "true"

  #############################################################################
  # Code Coverage (CPU only, on GitHub runner)
  #############################################################################

  code-coverage:
    name: Build Debug, Code Coverage
    runs-on: ubuntu-latest
    needs: [build-and-test-cpu]

    container:
      image: ghcr.io/aperijake/aperi-mech:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
      options: --user root

    env:
      # MPI Configuration for Docker Containers:
      # - OMPI_ALLOW_RUN_AS_ROOT: Required because GitHub Actions containers run as root user
      OMPI_ALLOW_RUN_AS_ROOT: 1
      OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: 1
      # - PRTE_MCA_rmaps: Allow oversubscription since container reports only 1 slot despite having 4 cores
      PRTE_MCA_rmaps_default_mapping_policy: :oversubscribe
      # - OMPI_MCA_btl: Disable shared memory transport to avoid container restrictions, use TCP loopback instead
      OMPI_MCA_btl: ^sm
      OMPI_MCA_btl_base_warn_component_unused: 0

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true
          submodules: false

      - name: Fix git ownership
        shell: bash
        run: |
          git config --global --add safe.directory $GITHUB_WORKSPACE

      - name: Install lcov
        shell: bash
        run: |
          apt-get update && apt-get install -y lcov

      - name: Configure and build with coverage
        shell: bash
        run: |
          set -e
          cd $GITHUB_WORKSPACE
          source /home/aperi-mech_docker/aperi-mech/venv/bin/activate
          spack env activate aperi-mech

          ./do_configure --build-type Debug --code-coverage

          # Build using all available cores
          # Note: Use 'nproc --all' instead of 'nproc' because containers may report limited quota
          # but actually have access to all host cores for build parallelism
          echo "Building with $(nproc --all) parallel jobs"
          cd build/Debug_cov
          make -j$(nproc --all)
          make coverage

      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ${{ github.workspace }}/build/Debug_cov/coverage.info
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}

  #############################################################################
  # Teardown
  #############################################################################

  teardown-gpu:
    runs-on: ubuntu-latest
    name: Teardown GPU VM (${{ matrix.vm_index }})
    needs: [setup-gpu, build-and-test-gpu]
    if: always()

    strategy:
      fail-fast: false
      max-parallel: 3 # VM_POOL_CONFIG: Deallocate all VMs in parallel (set to pool size)
      matrix:
        vm_index: [1, 2, 3] # VM_POOL_CONFIG: Update array to match setup-gpu matrix

    steps:
      - name: Deallocate Azure VM
        uses: azure/CLI@v2
        with:
          azcliversion: 2.62.0
          inlineScript: |
            echo "Logging into Azure..."
            az login --service-principal -u ${{ secrets.AZURE_CLIENT_ID }} -p ${{ secrets.AZURE_CLIENT_SECRET }} --tenant ${{ secrets.AZURE_TENANT_ID }}
            if [ $? -eq 0 ]; then
              echo "Logged in successfully"
            else
              echo "Failed to login"
              exit 1
            fi

            echo "Setting Azure subscription..."
            az account set --subscription ${{ secrets.AZURE_SUBSCRIPTION_ID }}
            if [ $? -eq 0 ]; then
                echo "Subscription set successfully"
            else
                echo "Failed to set subscription"
                exit 1
            fi

            # Determine VM name based on matrix index
            if [ "${{ matrix.vm_index }}" = "1" ]; then
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}"
            else
              VM_NAME="${{ secrets.AZURE_CICD_GPU_VM_BASE }}-${{ matrix.vm_index }}"
            fi

            echo "Checking status of VM: $VM_NAME"
            VM_STATUS=$(az vm get-instance-view --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME" --query instanceView.statuses[1].displayStatus --output tsv 2>/dev/null || echo "Not found")

            if [ "$VM_STATUS" = "VM running" ]; then
              echo "Deallocating VM $VM_NAME..."
              az vm deallocate --resource-group ${{ secrets.AZURE_RESOURCE_GROUP }} --name "$VM_NAME"
              if [ $? -eq 0 ]; then
                  echo "VM $VM_NAME deallocated successfully"
              else
                  echo "Failed to deallocate VM $VM_NAME"
                  exit 1
              fi
            else
              echo "VM $VM_NAME is not running (status: $VM_STATUS), skipping deallocation"
            fi

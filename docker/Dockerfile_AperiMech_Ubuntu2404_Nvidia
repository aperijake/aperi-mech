# Dockerfile for building a GPU-enabled environment for the aperi-mech project on Ubuntu 24.04

# Base image with CUDA support
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# Set the default shell to bash
SHELL ["/bin/bash", "-c"]

# Whether to pull the aperi-mech project or not
ARG PULL_APERI_MECH=false

# Number of processors to use for building
ARG NPROC=0

# Define a build argument for the CUDA architecture, defaulting to 75 (T4)
ARG CUDA_ARCH=75

# Validate PULL_APERI_MECH input
RUN PULL_APERI_MECH_LOWER="$(echo ${PULL_APERI_MECH} | tr '[:upper:]' '[:lower:]')" && \
    if [ "${PULL_APERI_MECH_LOWER}" != "true" ] && [ "${PULL_APERI_MECH_LOWER}" != "false" ]; then \
        echo "Error: PULL_APERI_MECH must be 'true' or 'false' (case insensitive), not '${PULL_APERI_MECH}'" && \
        exit 1; \
    fi && \
    echo "PULL_APERI_MECH set to: ${PULL_APERI_MECH_LOWER}"

# Store the lowercase version as an environment variable for later use
ENV PULL_APERI_MECH_VALIDATED="$(echo ${PULL_APERI_MECH} | tr '[:upper:]' '[:lower:]')"

# Calculate the number of processors to use and set it as an environment variable
RUN if [ "$NPROC" -eq 0 ]; then \
        NPROCS=$(nproc) && \
        if [ "$NPROCS" -gt 1 ]; then \
            echo "Setting processors to: $(expr $NPROCS - 1)" && \
            echo "NPROC_TO_USE=$(expr $NPROCS - 1)" > /tmp/nprocs; \
        else \
            echo "Setting processors to: 1" && \
            echo "NPROC_TO_USE=1" > /tmp/nprocs; \
        fi; \
    else \
        echo "Using specified processor count: $NPROC" && \
        echo "NPROC_TO_USE=$NPROC" > /tmp/nprocs; \
    fi && \
    cat /tmp/nprocs >> /etc/environment && \
    . /tmp/nprocs && \
    echo "NPROC_TO_USE = $NPROC_TO_USE"

ENV NPROC_TO_USE=${NPROC_TO_USE:-1}

# Validate CUDA_ARCH input
RUN if ! [[ "$CUDA_ARCH" =~ ^[0-9]+$ ]]; then \
        echo "Error: CUDA_ARCH must be a number, not '${CUDA_ARCH}'" && \
        exit 1; \
    fi && \
    # Check if architecture is supported
    if [[ ! " 60 61 62 70 72 75 80 86 87 89 90 " =~ " $CUDA_ARCH " ]]; then \
        echo "Warning: CUDA_ARCH=${CUDA_ARCH} might not be a valid architecture code." && \
        echo "Common values are:" && \
        echo "  60, 61, 62 (Pascal: P100, GTX 1080)" && \
        echo "  70, 72 (Volta: V100)" && \
        echo "  75 (Turing: T4, RTX 2080)" && \
        echo "  80, 86 (Ampere: A100, RTX 3080/3090)" && \
        echo "  87 (Ampere: RTX 3050/3060)" && \
        echo "  89 (Ada Lovelace: RTX 4090)" && \
        echo "  90 (Hopper: H100)"; \
    fi && \
    echo "CUDA_ARCH set to: ${CUDA_ARCH}"
# Set CUDA architecture
ENV CUDA_ARCH=${CUDA_ARCH}

# Avoid prompts from apt
ENV DEBIAN_FRONTEND=noninteractive

# Set CUDA environment variables
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
ENV PATH=${CUDA_HOME}/bin:${PATH}

# Simple verification of CUDA installation
RUN nvcc --version && \
    ls -l ${CUDA_HOME}/include/cuda.h && \
    ls -l ${CUDA_HOME}/lib64/libcudart.so && \
    echo "CUDA verification complete"

#################### System Packages from apt ####################
# Install necessary packages
# trunk-ignore(hadolint/DL3008)
# trunk-ignore(checkov/CKV2_DOCKER_1)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    ca-certificates \
    coreutils \
    curl \
    environment-modules \
    file \
    gfortran \
    git \
    git-lfs \
    gpg \
    htop \
    lcov \
    libcurl4-openssl-dev \
    libgl1 \
    libglu1-mesa \
    libssl-dev \
    lsb-release \
    openssh-client \
    openssl \
    python3 \
    python3-full \
    python3-venv \
    python3-pip \
    sudo \
    unzip \
    vim \
    xorg \
    zip \
    && rm -rf /var/lib/apt/lists/*

# Install NVIDIA utilities (nvidia-smi)
RUN apt-get update && \
    # Find the latest nvidia-utils-server package compatible with the installed CUDA
    NVIDIA_UTILS=$(apt-cache search nvidia-utils | grep -o "nvidia-utils-[0-9]*-server" | sort -V | tail -n 1) && \
    if [ -z "$NVIDIA_UTILS" ]; then \
        echo "No server version found, falling back to regular version" && \
        NVIDIA_UTILS=$(apt-cache search nvidia-utils | grep -o "nvidia-utils-[0-9]*" | grep -v "server" | sort -V | tail -n 1); \
    fi && \
    echo "Installing $NVIDIA_UTILS" && \
    apt-get install -y --no-install-recommends $NVIDIA_UTILS && \
    rm -rf /var/lib/apt/lists/*

#################### User Setup ####################
# Create a non-root user
RUN useradd -m aperi-mech_docker

# Switch back to root user
USER root

# Configure passwordless sudo for the user
RUN echo "aperi-mech_docker ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Change to the new user
USER aperi-mech_docker

# Make the working directory
WORKDIR /home/aperi-mech_docker

# Set the HOME environment variable
ENV HOME=/home/aperi-mech_docker

# Create a virtual environment
RUN python3 -m venv ${HOME}/venv

# Use the virtual environment for pip installations
RUN ${HOME}/venv/bin/pip install --no-input --no-cache-dir \
    pytest==8.3.2 \
    testbook==0.4.2 \
    jupyter==1.0.0 \
    jupyterlab==4.2.4 \
    numpy==2.0.1 \
    scipy==1.14.0 \
    matplotlib==3.9.1 \
    ipykernel==6.29.5 \
    meshio==5.3.5 \
    netCDF4==1.7.1.post1 \
    pandas==2.2.3 \
    && rm -rf ~/.cache/pip

# Add environment to Jupyter from the virtual environment
RUN ${HOME}/venv/bin/python -m ipykernel install --user --name aperi-mech --display-name "aperi-mech"

# Add virtual environment to the PATH
ENV PATH="${HOME}/venv/bin:${HOME}/.local/bin:$PATH"

#################### Spack Installation and Setup ####################
# Clone Spack repo
RUN git clone --depth=2 https://github.com/spack/spack.git ${HOME}/spack

# Set up Spack environment
ENV SPACK_ROOT=${HOME}/spack
ENV PATH="$SPACK_ROOT/bin:$PATH"
RUN . $SPACK_ROOT/share/spack/setup-env.sh

# Find compilers and externals for Spack
RUN spack compiler find && \
    spack external find

# Manually add CUDA to Spack packages.yaml
RUN . $SPACK_ROOT/share/spack/setup-env.sh && \
    mkdir -p ~/.spack && \
    echo "  cuda:" >> ~/.spack/packages.yaml && \
    echo "    externals:" >> ~/.spack/packages.yaml && \
    echo "    - spec: cuda@12.8.0" >> ~/.spack/packages.yaml && \
    echo "      prefix: /usr/local/cuda" >> ~/.spack/packages.yaml && \
    echo "    buildable: false" >> ~/.spack/packages.yaml

# Create a new Spack environment, cpu build
RUN spack env create aperi-mech

# Add packages to the Spack environment, aperi-mech
RUN . $SPACK_ROOT/share/spack/setup-env.sh && \
    spack -e aperi-mech add compadre@master ~tests && \
    spack -e aperi-mech add kokkos-kernels +cuda ~shared cuda_arch=${CUDA_ARCH} && \
    spack -e aperi-mech add kokkos +cuda +cuda_lambda +cuda_relocatable_device_code ~cuda_uvm ~shared +wrapper cxxstd=17 cuda_arch=${CUDA_ARCH} && \
    spack -e aperi-mech add trilinos@master ~amesos ~amesos2 ~anasazi ~aztec ~belos ~epetra ~epetraext ~ifpack ~ifpack2 ~ml ~muelu ~sacado ~shared +cuda +cuda_rdc +exodus +gtest +hdf5 +stk +zoltan +zoltan2 cxxstd=17 cuda_arch=${CUDA_ARCH} && \
    spack -e aperi-mech add googletest@1.14.0 && \
    spack -e aperi-mech add yaml-cpp@0.7.0 && \
    spack -e aperi-mech add eigen@master
# eigen@master is used as eigen@4.3 has a lot of GPU related warnings

# Install Packages with parallel builds
RUN . $SPACK_ROOT/share/spack/setup-env.sh && \
    spack -e aperi-mech install -j ${NPROC_TO_USE} --fresh

# Create a new Spack environment, for seacas. Want seacas without mpi, which causes conflicts with trilinos
RUN spack env create seacas

# Add packages to the Spack environment, seacas
RUN . $SPACK_ROOT/share/spack/setup-env.sh && \
    spack -e seacas add openmpi && \
    spack -e seacas add seacas ~mpi ~tests ~x11

# Install Packages, seacas, with parallel builds
RUN . $SPACK_ROOT/share/spack/setup-env.sh && \
    spack -e seacas install -j ${NPROC_TO_USE} --fresh

# Add the spack source command to the bashrc
RUN echo "source ${SPACK_ROOT}/share/spack/setup-env.sh" >> ${HOME}/.bashrc

ENV APERI_MECH_ENV="aperi-mech"
ENV APERI_MECH_GPU_ENV="aperi-mech"

# Set the working directory to the aperi-mech project
WORKDIR ${HOME}/aperi-mech

RUN if [ "${PULL_APERI_MECH_VALIDATED}" = "true" ]; then \
        echo "Building aperi-mech project"; \
        . $SPACK_ROOT/share/spack/setup-env.sh && \
        # Activate the Spack environment
        spack env activate ${APERI_MECH_ENV} && \
        # Download and install the aperi-mech project
        git clone --depth=2 https://github.com/aperijake/aperi-mech.git ${HOME}/aperi-mech && \
        # Only clone the repository - don't build or run tests as this would require GPU access during build
    else \
        echo "Skipping build of aperi-mech project"; \
    fi

# Set the default command to bash
CMD ["/bin/bash"]

# HEALTHCHECK to verify Spack and Python availability
HEALTHCHECK --interval=1m --timeout=10s --start-period=5s --retries=3 CMD /bin/bash -c "source ${SPACK_ROOT}/share/spack/setup-env.sh && python3 --version || exit 1"

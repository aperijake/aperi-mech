# syntax=docker/dockerfile:1.4

# Dockerfile for building a GPU-enabled environment for the aperi-mech project on Ubuntu 24.04

# Base image with CUDA support. Note that cuda 12.8 had ncurses linking issues in Trilinos/SEACAS/Aprepro
FROM nvidia/cuda:12.6.3-devel-ubuntu24.04

# Set the default shell to bash
SHELL ["/bin/bash", "-c"]

# Define a build argument for the CUDA architecture, defaulting to 75 (T4)
ARG CUDA_ARCH=75

# Define the user and group id
ARG USER_ID=1000
ARG GROUP_ID=1000

# Add build arg for protego (ssh submodules)
ARG PROTEGO=0

# Branch for the source code
ARG SRC_BRANCH=main

# Validate CUDA_ARCH input
RUN if ! [[ "$CUDA_ARCH" =~ ^[0-9]+$ ]]; then \
        echo "Error: CUDA_ARCH must be a number, not '${CUDA_ARCH}'" && \
        exit 1; \
    fi && \
    # Check if architecture is supported
    if [[ ! " 60 61 62 70 72 75 80 86 87 89 90 " =~ " $CUDA_ARCH " ]]; then \
        echo "Warning: CUDA_ARCH=${CUDA_ARCH} might not be a valid architecture code." && \
        echo "Common values are:" && \
        echo "  60, 61, 62 (Pascal: P100, GTX 1080)" && \
        echo "  70, 72 (Volta: V100)" && \
        echo "  75 (Turing: T4, RTX 2080)" && \
        echo "  80, 86 (Ampere: A100, RTX 3080/3090)" && \
        echo "  87 (Ampere: RTX 3050/3060)" && \
        echo "  89 (Ada Lovelace: RTX 4090)" && \
        echo "  90 (Hopper: H100)"; \
    fi && \
    echo "CUDA_ARCH set to: ${CUDA_ARCH}"
# Set CUDA architecture
ENV CUDA_ARCH=${CUDA_ARCH}

# Verify user and group IDs are numbers
RUN if ! [[ "$USER_ID" =~ ^[0-9]+$ ]] || ! [[ "$GROUP_ID" =~ ^[0-9]+$ ]]; then \
        echo "Error: USER_ID and GROUP_ID must be numbers, not '${USER_ID}' and '${GROUP_ID}'" && \
        exit 1; \
    fi && \
    echo "USER_ID set to: ${USER_ID}" && \
    echo "GROUP_ID set to: ${GROUP_ID}"

# Avoid prompts from apt
ENV DEBIAN_FRONTEND=noninteractive

# Set CUDA environment variables
ENV CUDA_HOME=/usr/local/cuda
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
ENV PATH=${CUDA_HOME}/bin:${PATH}

# Simple verification of CUDA installation
RUN nvcc --version && \
    ls -l ${CUDA_HOME}/include/cuda.h && \
    ls -l ${CUDA_HOME}/lib64/libcudart.so && \
    echo "CUDA verification complete"

#################### System Packages from apt ####################
# Install necessary packages
# trunk-ignore(hadolint/DL3008)
# trunk-ignore(checkov/CKV2_DOCKER_1)
RUN apt-get update && apt-get install -y --no-install-recommends \
    autoconf \
    automake \
    build-essential \
    ca-certificates \
    cmake \
    coreutils \
    curl \
    environment-modules \
    file \
    gfortran \
    git \
    git-lfs \
    gpg \
    htop \
    lcov \
    libaec-dev \
    libbz2-dev \
    libcurl4-openssl-dev \
    libgl1 \
    libglu1-mesa \
    libhwloc-dev \
    liblz4-dev \
    libncurses-dev \
    libopenblas-dev \
    libsnappy-dev \
    libssl-dev \
    libtool \
    libxml2-dev \
    libzstd-dev \
    lsb-release \
    openssh-client \
    openssl \
    pkgconf \
    python3 \
    python3-full \
    python3-venv \
    python3-pip \
    sudo \
    unzip \
    vim \
    xorg \
    zip \
    && rm -rf /var/lib/apt/lists/*

# Install NVIDIA utilities (nvidia-smi)
RUN apt-get update && \
    # Find the latest nvidia-utils-server package compatible with the installed CUDA
    NVIDIA_UTILS=$(apt-cache search nvidia-utils | grep -o "nvidia-utils-[0-9]*-server" | sort -V | tail -n 1) && \
    if [ -z "$NVIDIA_UTILS" ]; then \
        echo "No server version found, falling back to regular version" && \
        NVIDIA_UTILS=$(apt-cache search nvidia-utils | grep -o "nvidia-utils-[0-9]*" | grep -v "server" | sort -V | tail -n 1); \
    fi && \
    echo "Installing $NVIDIA_UTILS" && \
    apt-get install -y --no-install-recommends $NVIDIA_UTILS && \
    rm -rf /var/lib/apt/lists/*

#################### User Setup ####################
# Check if UID exists and handle accordingly, modify if exists, create if not. Can be set to match host for easy file sharing
RUN if getent passwd ${USER_ID} > /dev/null; then \
      echo "UID ${USER_ID} already exists, modifying existing user" && \
      existing_user=$(getent passwd ${USER_ID} | cut -d: -f1) && \
      usermod -l aperi-mech_docker $existing_user && \
      groupmod -n aperi-mech_docker $(id -gn $USER_ID) && \
      usermod -d /home/aperi-mech_docker -m aperi-mech_docker; \
    else \
      # Create the group first if it doesn't exist
      if ! getent group ${GROUP_ID}; then \
        groupadd -g ${GROUP_ID} aperi-mech_docker; \
      fi && \
      useradd -u ${USER_ID} -g ${GROUP_ID} -m aperi-mech_docker; \
    fi

# Switch back to root user
USER root

# Configure passwordless sudo for the user
RUN echo "aperi-mech_docker ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# Make the working directory
WORKDIR /home/aperi-mech_docker

# Set the HOME environment variable
ENV HOME=/home/aperi-mech_docker

# Set the default command to bash
CMD ["/bin/bash"]

# Clone the repo from GitHub, using the specified branch
RUN git clone https://github.com/aperijake/aperi-mech.git ${HOME}/aperi-mech && \
    cd ${HOME}/aperi-mech && git checkout "$SRC_BRANCH"

# Fail early if PROTEGO is enabled but no SSH agent is detected
RUN --mount=type=ssh \
    if [ "$PROTEGO" = "1" ]; then \
      if [ ! -S "$SSH_AUTH_SOCK" ]; then \
        echo "PROTEGO enabled but no SSH agent detected. Failing build."; \
        exit 1; \
      fi; \
    fi

# Add GitHub to known_hosts to avoid host key verification errors
RUN mkdir -p /root/.ssh && \
    ssh-keyscan github.com >> /root/.ssh/known_hosts

# Optionally update submodules if SSH agent is available and PROTEGO is set
RUN --mount=type=ssh \
    if [ "$PROTEGO" = "1" ]; then \
      cd ${HOME}/aperi-mech && \
      echo "PROTEGO enabled and SSH agent detected. Updating submodules with SSH..."; \
      git submodule update --init --recursive; \
    else \
      echo "PROTEGO not enabled. Skipping submodule update."; \
    fi

# Ensure the aperi-mech directory is owned by the aperi-mech user
RUN chown -R aperi-mech_docker:aperi-mech_docker ${HOME}/aperi-mech

# Change to the new user
USER aperi-mech_docker

# Change to the cloned repository directory
WORKDIR ${HOME}/aperi-mech

# Automatically activate the venv for interactive shells
RUN echo "source ${HOME}/aperi-mech/venv/bin/activate" >> ${HOME}/.bashrc

# Set Fortran compiler and aperi-mech environment variables globally
ENV APERI_MECH_ROOT=${HOME}/aperi-mech
ENV FC=/usr/bin/gfortran 
ENV F77=/usr/bin/gfortran

# Set up Python virtual environment and install required tooling
RUN python3 -m venv venv && \
    . ./venv/bin/activate && \
    echo "export APERI_MECH_ROOT=$APERI_MECH_ROOT" >> $APERI_MECH_ROOT/venv/bin/activate && \
    cd $APERI_MECH_ROOT/tools/python/aperi-mech && \
    python3 -m pip install -e . && \
    cd $APERI_MECH_ROOT/tools/python/spackx && \
    python3 -m pip install -e .

# Install Spack and add it to the virtual environment
RUN mkdir -p $APERI_MECH_ROOT/venv/src && \
    cd $APERI_MECH_ROOT/venv/src && \
    git clone https://github.com/spack/spack.git && \
    git -C $APERI_MECH_ROOT/venv/src/spack checkout prereleases/v1.0.0-alpha.4 && \
    echo "source $APERI_MECH_ROOT/venv/src/spack/share/spack/setup-env.sh" >> $APERI_MECH_ROOT/venv/bin/activate

# Configure the Spack environment
RUN . ./venv/bin/activate && \
    spack compiler find && \
    spack external find && \
    spack external find cmake hwloc libxml2 pkgconf bzip2 lz4 snappy zstd libaec ncurses openblas autoconf automake libtool

# Patch Spack for Trilinos support
RUN . ./venv/bin/activate && \
    spack patch-trilinos

# Manually add CUDA to Spack packages.yaml
RUN . ./venv/bin/activate && \
    CUDA_VER=$(nvcc --version | grep "release" | sed 's/.*release //' | awk '{print $1}' | cut -d',' -f1) && \
    mkdir -p ~/.spack && \
    echo "  cuda:" >> ~/.spack/packages.yaml && \
    echo "    externals:" >> ~/.spack/packages.yaml && \
    echo "    - spec: cuda@${CUDA_VER}" >> ~/.spack/packages.yaml && \
    echo "      prefix: /usr/local/cuda" >> ~/.spack/packages.yaml && \
    echo "    buildable: false" >> ~/.spack/packages.yaml

# Create a Spack environment for aperi-mech and install it
RUN . ./venv/bin/activate && \
    spack env create aperi-mech && \
    spack -e aperi-mech develop --path $APERI_MECH_ROOT aperi-mech && \
    if [ "$PROTEGO" = "1" ]; then \
      spack -e aperi-mech add aperi-mech +protego +cuda cuda_arch=${CUDA_ARCH} build_type=Release; \
    else \
      spack -e aperi-mech add aperi-mech +cuda cuda_arch=${CUDA_ARCH} build_type=Release; \
    fi && \
    spack -e aperi-mech concretize -f

# Just breaking up and installing the dependencies in a few steps to give a docker build cache
RUN . ./venv/bin/activate && \
    spack -e aperi-mech install eigen yaml-cpp googletest

RUN . ./venv/bin/activate && \
    spack -e aperi-mech install kokkos kokkos-kernels compadre

RUN . ./venv/bin/activate && \
    spack -e aperi-mech install trilinos

## This step is currently failing when it tries to test run an executable as part of the build process
##  - tried to disable the test run, but could not find a way to do that
##  - suggestion is to do this last step manually after the container is built. That is:
##       - start the container, for example: docker run --gpus all -it aperi-mech:cuda-t4
##       - source the virtual environment: source $APERI_MECH_ROOT/venv/bin/activate
##       - install aperi-mech: spack -e aperi-mech install aperi-mech
# RUN . ./venv/bin/activate && \
#     spack -e aperi-mech install aperi-mech

# Set environment variables for aperi-mech
ENV APERI_MECH_ENV="aperi-mech"
ENV APERI_MECH_GPU_ENV="aperi-mech"

# Register the local aperi-mech Spack repository so the aperi-mech repo can be found for future builds
RUN . ./venv/bin/activate && \
    spack -e aperi-mech repo add $APERI_MECH_ROOT/tools/python/spackx/src/spackx/repos/aperi/

# When running the container:
# To get the build location of aperi-mech, you can run:
#   spacktivate aperi-mech
#   spack location -i aperi-mech build_type=Release
# Then cd to the bin folder in that directory to run the unit tests
# Same for the Debug build

# To build aperi-mech with build_type=Debug in the same environment, in the container you can run:
#   spacktivate aperi-mech && \
#   spack config edit
#    change concretizer:unify to false # enables multiple builds in the same environment
#   spack add aperi-mech build_type=Debug 
#   spack install build_type=Debug
# May get some error about the environment view, but it seems to not be an issue

# Or, you can use the do_configure script to build manually

# Generic healthcheck to ensure the container is running
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD [ "bash", "-c", "echo 'Container is healthy'" ]
